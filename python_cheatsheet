==== Basic Library ====
import pandas # Import a package without an alias
import pandas as pd # Import a package with an alias
from pandas import DataFrame # Import an object from a package


==== Python Basic Learning Note ====


*** Tricks ***
--- COMMENTS
cmd+/ -> comments all

"""
"""   -> comment all paragraphs

--- Last digit of a number
a % 10

--- copy a list 
result = sentence[:]

% # magic 

--- print a string with variable value
print(f"you have {cheese_count} cheeses!")

---
p = []
print(*p)

---
magic functions
% matplotlib inline

pd.options.display.max_rows = 99

------------------------------------------------------------------------------------------------------------------------------------------------------------------------
*** Useful functions ***

map(function,iterable) - return list
--- map(square,[1,2,3,4,5])

str() - convert to string
--- str(a)

type() - check the variable type

range() - generate a list
range(3) gives [0,1,2]
range(0,20,2) gives [0,2,4,6...18]
range(5,0,-1) gives [5,4,3,2,1]

Lambda - 
--- lambda x:x*2
--- map(lambda x : x['name'],dict)

filter() - return list
--- filter(function_object, iterable)

------------------------------------------------------------------------- Python Common Data Type  -----------------------------------------------------------------------------------------------
*** Variables ***
tuples - cannot change value, but the cell can change value 
--- tup = (4,5,6)
a,b,c = tup

list - can change size and can be different values.
--- function: append,insert,pop,in,extend,sort,
--- seq[1:5], seq[:5],seq[1:],seq[-1:],seq[-6,-2],seq[::2]
--- inverse the list: seq[::-1]

dictionary - with key and value
--- dict = {}
---mapping = {} - to construct a dictionary
for i,v in enumerate(some_list):
	mapping[v] = i
--- function pop,del
--- dict.keys(),dict.values() - to find dictionary key and value

fast expression
--- list = [x.upper() for x in strings if len(x)>2]
--- dict = {len}

Series
obj = pd.Series([1,2,3,4,5])
obj2 = pd.Series([1,2,3,4,5], index = [])


to construct a dataframe
data = pd.DataFrame(np.arrange(6).reshape((2,3)),
					index = pd.Index(['A','B'],name = 'state'),
					columns = pd.Index(['one','two','three'], name = 'number'))


----------------------------------------------------------------------- Python In General -------------------------------------------------------------------------------------------------
print(f"you have {cheese_count} cheeses!")

# string
str_var = 'AAAA'

# increment
x += 1 -> x= x+1

# if statement
if people > cats:
	print("statements_A")
elif people < cats:
	print("statement_B")
else:
	print("statement_C")


--- for loop 
for i in list:
	print(f "it is {i}")

for i in range(0,10):
	print(f"ssss")

for i in range(4):
	print(i)

--- while loop
i = 0 
while i <10:
	print("something")
	i +=1

# functions
def function():
	print("anything")

def return_fun(a,b):
	return a+b

def function(a):
	return sorted(a)

# MODULES
# mystuff.py
def apple():
	print("this is an apple")
---------------
import mystuff
mystuff.affple()

# CLASS
class Song(object):
	# object constructor
	def _init_(self,lyrics):
		self.lyrics = "lyrics"
	# method
	def sing_me_a_song(self):
		for line in self.lyrics:
			print(line)

# CREATE OBJECT
parade = Song(["A","B","C"])


--------------- functions --------------- 
1. 
def polygon_perimeter(n_sides, side_len):
def polygon_apothem(n_sides, side_len):
def polygon_area(n_sides, side_len):
    perimeter = polygon_perimeter(n_sides, side_len)
    apothem = polygon_apothem(n_sides, side_len)
	return perimeter * apothem / 2
2. 
--------------------------------------------------------------------------- Library ---------------------------------------------------------------------------------------------


******************************************************************************* NUMPY *******************************************************************************

构建数组，数组中元素类型相同
data = np.random.randn(2,3)
data = np.arange(15)

FUNCTIONS:

--- .array(list)
np.array(list)

--- .astype(np.float64)

--- np.arrange(10) - randomly generate 1 dimensional array

--- .copy() - completely generate a same np list

--- data[data<0] = 0

--- .reshape()
np.arange(10).reshape((3,5))

--- arr.sum(),arr.mean(),arr.

--- np.where()

******************************************************************************* PANDAS ********************************************************************************

import pandas as pd
from pandas import Series, DataFrame
series = pd.Series() -- series contains index and value
series.index, series.value, series.name
SERIES FUNCTIONS:
--- .isnull()
pd.isnull(series)


df = pd.DatatFrame(data) -- dataframe can be constructured by numpy or dictionary. It contains index and value
df = pd.DataFrame(data,columns = ['A','B','C','D'],index = ['one','two','three'])
DATAFRAME FUNCTIONS:
--- index method: df['A'], df.A, df.columns, df.columns.name, df.T, df.index, df.index.name, df.values
--- del : delete column
e.g.
del df['A']
--- index and filtering: data[data['A']>9]
--- loc & iloc
loc select column and row name, iloc select the index from the dataframe
e.g.
data.iloc[:,:3][data.three>5]
--- apply() : only for column or row
df.apply(f,axis = 'colimns')
--- applymap() : for every data in the frame
*--- map(): ONLY for series
--- .drop()
df.drop(['A','B'],axis=1,inplace = True)
--- .sort_index() : sort the dataframe by index
df.sort_index(axis=1,ascending = False)
--- .index.is_unique : to find out if dataframe index is unique
--- .sum()
df.sum(axis='columns')
--- .mean()
df.mean(axis='columns',skipna = False)
--- .idxmax(), idxmin() : return the index of the max,min values
df.idmax()
--- unique() : classify the value
df.unique()


******************************************************************************* Parallel Computing& Big Data Technique ********************************************************************************
# the Pool library 
@print_timing
def parallel_apply(apply_func, groups, nb_cores):
    with Pool(nb_cores) as p:
        results = p.map(apply_func, groups)
    return pd.concat(results)

# Dask
import dask.dataframe as dd
# Set the number of pratitions
athlete_events_dask = dd.from_pandas(athlete_events, npartitions = 4)
# Calculate the mean Age per Year
print(athlete_events_dask.groupby('Year').Age.mean().compute())


# Apache Pipeline
# Create the DAG object
dag = DAG(dag_id="car_factory_simulation",
          default_args={"owner": "airflow","start_date": airflow.utils.dates.days_ago(2)},
          schedule_interval="0 * * * *")

etl_task = PythonOperator(task_id = "etl_task",python_callable = etl,dag=dag)
etl_task.set_upstream(wait_for_this_task)


# Task definitions
assemble_frame = BashOperator(task_id="assemble_frame", bash_command='echo "Assembling frame"', dag=dag)
place_tires = BashOperator(task_id="place_tires", bash_command='echo "Placing tires"', dag=dag)
assemble_body = BashOperator(task_id="assemble_body", bash_command='echo "Assembling body"', dag=dag)
apply_paint = BashOperator(task_id="apply_paint", bash_command='echo "Applying paint"', dag=dag)

# Complete the downstream flow
assemble_frame.set_downstream(place_tires)
assemble_frame.set_downstream(assemble_body)
assemble_body.set_downstream(apply_paint)




******************************************************************************* Database ETL ********************************************************************************+++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  Extract Data ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
# SQL database ETL
# Function to extract table to a pandas DataFrame
def extract_table_to_pandas(tablename, db_engine):
    query = "SELECT * FROM {}".format(tablename)
    return pd.read_sql(query, db_engine)

# Connect to the database using the connection URI
connection_uri = "postgresql://repl:password@localhost:5432/pagila" 
db_engine = sqlalchemy.create_engine(connection_uri)
user1 = pd.read_sql("SELECT * FROM rating WHERE user_id = 4387", db_engine)

# Extract the film table into a pandas DataFrame
extract_table_to_pandas("film", db_engine)

# Extract the customer table into a pandas DataFrame
extract_table_to_pandas("customer", db_engine)


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  Transform Data & Pyspark ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
import pyspark.sql
spark = pyspark.sql.SparkSession.builder.getOrCreate()
spark.read.jdbc("jdbc:postgresql://localhost:5432/pagila",
                "customer",
                {"user":"repl","password":"password"})

- Split Data

# Get the rental rate column as a string
rental_rate_str = film_df.rental_rate.astype(str)
# Split up and expand the column
rental_rate_expanded = rental_rate_str.str.split('.', expand=True)
# Assign the columns to film_df
film_df = film_df.assign(
    rental_rate_dollar=rental_rate_expanded[0],
    rental_rate_cents=rental_rate_expanded[1],
)

# Use groupBy and mean to aggregate the column
ratings_per_film_df = rating_df.groupBy('film_id').mean('rating')

# Join the tables using the film_id column
film_df_with_ratings = film_df.join(
    ratings_per_film_df,
    film_df.film_id==ratings_per_film_df.film_id
)
# Show the 5 first results
print(film_df_with_ratings.show(5))

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  Loading ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
# load to the column database
df.to_parquet()
df.write.parquet()

# load to the row database (AMAZON REDSHIFT S3)
recommendations = transform_find_recommendatins(ratings_df)
recommendations.to_sql("recommendations",db_engine,schema="store",if_exists = "replace")

# Finish the connection URI
connection_uri = "postgresql://repl:password@localhost:5432/database"
db_engine_dwh = sqlalchemy.create_engine(connection_uri)
# Transformation step, join with recommendations data
film_pdf_joined = film_pdf.join(recommendations)
# Finish the .to_sql() call to write to store.film
film_pdf_joined.to_sql("film", db_engine_dwh, schema="store", if_exists="replace")
# Run the query to fetch the data
pd.read_sql("SELECT film_id, recommended_film_ids FROM store.film", db_engine_dwh)


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  ETL table ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
# extract data from the original database 




# Loading data back to database
connection_uri = "postgresql://repl:password@localhost:5432/dwh"
db_engine = sqlalchemy.create_engine(connection_uri)
def load_to_dwh(recommendations):
    recommendations.to_sql("recommendations", db_engine, if_exists=="replace")




# Creating the DAG





Porject A ==============================================================================================================================

# identify the maximum and minimum value of the identified variable 
print(df['Total Consumer Charges'].astype(float).min())
print(df['Total Consumer Charges'].astype(float).max())


# data tranformation & remove the semi coolums
df['Total Payments Received'] = df['Total Payments Received'].astype(str)
df['Total Payments Received'] = df['Total Payments Received'].apply(lambda x:x.replace(',',''))
df['Total Payments Received']

# define the segment range for the variable
df['Total Merchant Charges'] = df['Total Merchant Charges'].astype(float)
df['tmc_Seg_Level'] = df['Total Merchant Charges'].astype(int)
df['tmc_Seg_Level'] = df['tmc_Seg_Level'].apply(of_Seg)
df['tmc_Seg_Level']

# find the precentage of the variable 
df_OF_Pct = df[['OF_Seg_Level','Other Fees']]
df_OF_Pct = df_OF_Pct.groupby('OF_Seg_Level').size()
df_OF_Pct = pd.DataFrame({'OF_Seg_Level':df_OF_Pct.index,'Quality':df_OF_Pct.values})
df_OF_Pct['Percent'] = df_OF_Pct['Quality']/276546
df_OF_Pct

# plot
ax = sns.barplot(x = 'Signed_Year', y = 'Quality', data = df_ContbyYear)
ax.set_title('App pricing trend across categories')
ax.set_xticklabels(ax.get_xticklabels(), rotation=45)


Tricks ==============================================================================================================================
df_main['SALE_PRICE'] =df_main.apply(lambda x:price_calc(x['PDT_MODEL'],x['TOTAL_LENGTH'],x['TOTAL_QTY']),axis=1)

EDA

==== Python EDA & WrangleCheatsheet ====

files_xlsx = [f for f in files if f[-4:] == 'xlsx']

# Remove all columns 
df.drop(df.iloc[:, 1:3], inplace = True, axis = 1) 

# print list 
cols = df.columns.tolist()

# rename columns
df.rename(columns={"A": "a", "B": "c"})


# empty columns
df["C"] = ""
df["D"] = np.nan



# Dataframe reset index 
df = df.reset_index(drop=True)

# df rename the index
df.index.names = ['Date']

# df .values convert the dataframe to array
df.values

# create a dataframe 
df = pd.DataFrame({'A': [11, 21, 31],
                   'B': [12, 22, 32],
                   'C': [13, 23, 33]},
                  index=['ONE', 'TWO', 'THREE'])

# drop a column , 1 for column, 0 for row.
df = df.drop('column_name', 1)
df.drop(['column_nameA', 'column_nameB'], axis=1, inplace=True)



# replace the tuples in the column (A is column name)
df.A.replace(['MM','NN'],[’OO‘,’PP‘]，inplace = True)

# change data type
df.A.astype(int)

# rename column
df.rename(columns = {'A':'BC'}, inplace = True)

---------------------------------------------------------------- iloc & loc funct ---------------------------------------------------------------- 

##### iloc function - use row amd column index number to find the value
data.iloc[[1],[0]]
data.iloc[:,[0]]

# Single selections using iloc and DataFrame
# Rows:
data.iloc[0] # first row of data frame (Aleshia Tomkiewicz) - Note a Series data type output.
data.iloc[1] # second row of data frame (Evan Zigomalas)
data.iloc[-1] # last row of data frame (Mi Richan)
# Columns:
data.iloc[:,0] # first column of data frame (first_name)
data.iloc[:,1] # second column of data frame (last_name)
data.iloc[:,-1] # last column of data frame (id)

# Multiple row and column selections using iloc and DataFrame
data.iloc[0:5] # first five rows of dataframe
data.iloc[:, 0:2] # first two columns of data frame with all rows
data.iloc[[0,3,6,24], [0,5,6]] # 1st, 4th, 7th, 25th row + 1st 6th 7th columns.
data.iloc[0:5, 5:8] # first 5 rows and 5th, 6th, 7th columns of data frame (county -> phone1).


#####loc function - use row and column name to locate the value

# Select rows with index values 'Andrade' and 'Veness', with all columns between 'city' and 'email'
data.loc[['Andrade', 'Veness'], 'city':'email']

# Select same rows, with just 'first_name', 'address' and 'city' columns
data.loc['Andrade':'Veness', ['first_name', 'address', 'city']]

# Select rows with first name Antonio, # and all columns between 'city' and 'email'
data.loc[data['first_name'] == 'Antonio', 'city':'email']
 
# Select rows where the email column ends with 'hotmail.com', include all columns
data.loc[data['email'].str.endswith("hotmail.com")]   
 
# Select rows with last_name equal to some values, all columns
data.loc[data['first_name'].isin(['France', 'Tyisha', 'Eric'])]   
       
# Select rows with first name Antonio AND hotmail email addresses
data.loc[data['email'].str.endswith("gmail.com") & (data['first_name'] == 'Antonio')] 
 
# select rows with id column between 100 and 200, and just return 'postal' and 'web' columns
data.loc[(data['id'] > 100) & (data['id'] <= 200), ['postal', 'web']] 
 
# A lambda function that yields True/False values can also be used.
# Select rows where the company name has 4 words in it.
data.loc[data['company_name'].apply(lambda x: len(x.split(' ')) == 4)] 
 
# Selections can be achieved outside of the main .loc for clarity:

# Form a separate variable with your selections:
idx = data['company_name'].apply(lambda x: len(x.split(' ')) == 4)

# Select only the True values in 'idx' and only the 3 columns specified:
data.loc[idx, ['email', 'first_name', 'company']]

#


##### dataframe cut function - to selgement data column
df["new"] = pd.cut(df["old"], cut-points,labels = label_names)

===============================================================================================================
				########################        Regular Expression Clean        ########################
===============================================================================================================
import re

def clean_strings(strings):
	result = []
	for value in strings:
		value = value.strip()
		value = re.sub()
		value = value.title()
		result.append(value)
	return result

for x in map(clean_strings,states)
	print(x)


===============================================================================================================
				########################        Popular Library        ########################
===============================================================================================================
matplotlib # graph and data visualisation 
json #jason library
# pandas
import pandas as pd
# numpy
import numpy as np
# plot lib
import matplotlib.pyplot as plt
%matplotlib inline
===============================================================================================================
				########################        Important Function        ########################
===============================================================================================================
# lambda function & apply function
# apply function and lambda function (A is column name)
df[’A‘] = df['A'].apply(lambda x: round(x/1000))

# locate specific cell in a column
.iloc[]

# split function
.split(',')
split_email = customer_df.email.str.split('@', expand=True)
customer_df = customer_df.assign.(
	username= split_email[0],
	domian = split_email[1],
)

# aggregate function
df.agg(['sum', 'min'])
df.agg("mean", axis="columns")
===============================================================================================================
				########################        File Prasing        ########################
===============================================================================================================
# CSV
.read_csv()  # read csv file
type()   # define the type of the variable

# JSON
data = json.loads()  # load jason file
.read_json()   # read jason file 
json_normalize()  # turn out to dataframe format

# XML
# using element tree library
import xml.etree.ElementTree as etree    
tree = etree.parse("./Melbourne_bike_share.xml") 
featurename = [] 
for elem in tree.iter(tag = 'featurename'):
   featurename.append(elem.text) 
dataDict = {} # Empty Dataframe
dataDict['Featurename'] = featurename # Add column into the dataframe
# using Beautiful Soup

# EXCEL
import pandas as pd
excel_data = pd.ExcelFile('SOWC 2014 Stat Tables_Table 2.xlsx')
excel_data.sheet_names
df = excel_data.parse('Table 2 ')


===============================================================================================================
						########################        EDA        ########################
===============================================================================================================
# Check HEAD and TAIL
df.tail()
df.head()


# classify and count how many values for each
df['A'].value_counts()
df["X"].value_counts().index
df["X"].value_counts().values

# Check the columns Classification
pd.unique(df['A'])
# df['A'].unique

# check value type
df['A'].dtype

#check the column size
df['A'].size

# LENGTH of dataframe
len()

# DF SHAPE
.shape

# SEARCH function
df[df.A = "aaaa"]


# DATAFRAME
.set_index(csvdf.ID, inplace = True)
.to_datetime()  # set the date format

# SORT LIST
.sort()

# GROUPBY to check the DATAFRAME same attribute 
.groupby()

# SEARCH function
def search(this, that): # look for this in that
    print ("\n".join(s for s in that if this in s))



# Attribute Info & general discription
.info()
.describe()



# Check the Classification Values (How many each type)
df['A'].value_counts()

# Check NULL Value
sum(df['A'].isnull())

# Extract contain 
df['A'].str.extract('')
df['A'].str.contains("AAA")==False

# find index of the tuple
.index

===============================================================================================================
						########################     Graphic EDA     ########################
===============================================================================================================


===============================================================================================================
						########################     Data Wrangling     ########################
===============================================================================================================
### Data Source
AWS public data set

### online wrangler tools
􏰃􏰅􏰅􏰏􏰧􏰹􏰹􏰘􏰆􏰇􏰗􏰇􏰅􏰄􏰊􏰞􏰖􏰉􏰑􏰗􏰐􏰑􏰓􏰹􏰙􏰉􏰄􏰊􏰋􏰌􏰐􏰉􏰹􏰃􏰅􏰅􏰏􏰧􏰹􏰹􏰘􏰆􏰇􏰗􏰇􏰅􏰄􏰊􏰞􏰖􏰉􏰑􏰗􏰐􏰑􏰓􏰹􏰙􏰉􏰄􏰊􏰋􏰌􏰐􏰉􏰹􏰃􏰅􏰅􏰏􏰧􏰹􏰹􏰘􏰆􏰇􏰗􏰇􏰅􏰄􏰊􏰞􏰖􏰉􏰑􏰗􏰐􏰑􏰓􏰹􏰙􏰉􏰄􏰊􏰋􏰌􏰐􏰉􏰹http://vis/stanford.edu/wrangler/

###Library
NLTK - natural language processing
Pandas

+++++++++++++++++++++++++++++++++++++++  N/A processing  +++++++++++++++++++++++++++++++++++++++
# info() function can show how many null values
.info()
.isnull.sum()


# Check Columns
df[df['A']=="aaaa"]

# CHECK Null Value in dataframe
df[df['A'].isnull()]

# check the sum of the null value
df['A'].isnull().sum()
s.isnull.values.any()

data.apply(lambda x: sum(x.isnull()),axis=0)
data.apply(lambda x: sum(x.isnull()),axis=1)

# REMOVE Null 
df = df.dropna(0, how = 'all')  # 0 means row
df = df.dropna(1, how = 'all')	# 1 means columns
df = df.drop()
# DROPNA by A-B columns
merged_df = merged_df.dropna(subset=['col_A', 'col_B'])

# find MEAN VALUE
mean = np.mean(df.A)

# fill with MEAN VALUE
df.A = df.A.fillna(mean)

# fill with value By column
df.fillna({1:0.5,2:0}) - the first column fill with 0.5 and the second column file with 0
# fill na by column
values = {'A': 0, 'B': 1, 'C': 2, 'D': 3}
df.fillna(value=values)

### Set Index
df.index = xrange(len(df.index))


# plot the missing values
missing = train.isnull().sum()
missing = missing[missing > 0]
missing.sort_values(inplace=True)
missing.plot.bar()


+++++++++++++++++++++++++++++++++++++++  duplication  +++++++++++++++++++++++++++++++++++++++

# FIND duplicated value
df.duplicated()

# REMOVE duplicated value (whole dataset)
df.drop_duplicates()

# REMOVE duplicated value by column
df.drop_duplicates([k1],[k2])
+++++++++++++++++++++++++++++++++++++++  rename & replace  +++++++++++++++++++++++++++++++++++++++
# REPLACE VALUE
data.replace(-1,333)
data.replace([-1,-2],333)
data.replace({-1:333,-2:444})

# RENAME the index
data.rename(index = {'OHIO':'INDIANA'},columns ={'three','peekaboo'})

+++++++++++++++++++++++++++++++++++++++  filter the dataframe  +++++++++++++++++++++++++++++++++++++++

df[(np.abs(data)>3).any(1)]
# filter the numbers
super_bowls[super_bowls['difference_pts']>=35]
# filter the strings
no_bands = halftime_musicians[~halftime_musicians.musician.str.contains('Marching')]
# isin
popular_app_cats = apps[apps.Category.isin(['GAME', 'FAMILY', 'PHOTOGRAPHY',
                                            'MEDICAL', 'TOOLS', 'FINANCE',
                                            'LIFESTYLE','BUSINESS'])]
+++++++++++++++++++++++++++++++++++++++  Deal with string +++++++++++++++++++++++++++++++++++++++
str = ''

str.strip() -- remove space
str.split() -- split by 
str.join(piece)

df.str.findall()
df.str.match()


+++++++++++++++++++++++++++++++++++++++ Outliers Processing +++++++++++++++++++++++++++++++++++++++
# detect the outliers by using boxplot
bp = df1.boxplot(column='fare')


### Bias processing

### Regular Expressions

+++++++++++++++++++++++++++++++++++++++ Integration Processing +++++++++++++++++++++++++++++++++++++++
# Multi-level index frame
data.sort_index(level=0)

# merge data using .merge() : (default merge is the same as the inner join,delecting the uncommon data from the result list)
pd.merge(df1,df2,on ='key')
pd.merge(df1,df2,how = 'outer')
pd.merge(df1,df2,left_on='key',right_index=True,how='outer')

# merge data using .concat() 
df = pd.concat([df1,df2])
df = pd.concat([df1,df2], axis  =1, keys = ['level1','level2'])

# use concat performing outer joiner
result = pd.concat([df1, df4], axis=1, sort=False)
# use concat performaing inner joiner
result = pd.concat([df1, df4], axis=1, join='inner')

df1.combine_first(df2)

#insert new columns 
df.insert(2, "Age", [21, 23, 24, 21], True) 
#insert new columns and create new dataframe
df2 = df.assign(address = ['Delhi', 'Bangalore', 'Chennai', 'Patna']) 

+++++++++++++++++++++++++++++++++++++++ Duplication +++++++++++++++++++++++++++++++++++++++
# check duplication
cols = df.columns.differnece([])
duplicateRecords = df[(df.duplicated(cols,keep=False))]
len (duplicateRecord)

# remove the duplicate value in particular column 
data.drop_duplicates(subset ="First Name", keep = False, inplace = True)

# drop duplicate
df = df.drop_duplicates(cols, keep='last')
df.shape

+++++++++++++++++++++++++++++++++++++++ Pivot Funct +++++++++++++++++++++++++++++++++++++++

#### pivot funtion (Change the column and row oriantation) - make sure the entries for index would not duplicated
df.pivot(index='foo', columns='bar', values=['baz', 'zoo'])

result = data.stack() -- switch columns to rows
data.unstack() -- restore, normaly reconstruct the dataframe with only one key column


# column to row
pivot_data = data.pivot('date','item','value') -- need to define the key, row, column in this method. especially when the key is not unique. AND the value calculated is the average value. We need agg function to define others such as sum, 
pd.pivot_table(df,index=["Manager","Rep"],values=["Price"],aggfunc=np.sum)
# row to column
melted_data = pd.melt(data,['key'])

===============================================================================================================
						########################     Pandas ETL     ########################
===============================================================================================================
# group the data
df.groupby(['key1','key2']).mean()
df.groupby(['key1','key2']).size() 
df.groupby([]).min()
df.groupby([]).count()
df.groupby([]).filter()

# aggregate function with groupby 
goupy = df.groupy('key1','key2')
functions = ['count','mean','max']
res = group['col1','col2'].agg(functions)

# crosstab -- used to calculate the frequency for categories number
df
pd.crosstab(df.A,df.B,margins = True)

===============================================================================================================
						########################     Time Series     ########################
===============================================================================================================
Library - datetime
from datetime import datetime 
Library - parser.parse

now
now = datetime.now()
now.year now.month now.day

format
timestamp = datetime(2020,5,30)
str(timestamp)
timestamp.strftime('%Y - %m - % %d')

standards formats
parse('2020-02-03')

pandas change to datetime
pd.to_datetime(datestrs)

reset the frequency
resampler = time.resample('D')

index_date = pd.date_range('2012-04-01','2012-05-01')

timestemp.shift(2,freq = 'D')




++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
						########################     Practical hints     ########################
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# change string to numbers
pd.to_numeric

# change type 
astype()
.astype(int)
.astype(str)

===============================================================================================================
						########################     Reference     ########################
===============================================================================================================
如何理解boxplot
https://zhuanlan.zhihu.com/p/110580568


